[
  {
    "objectID": "index.html#overview",
    "href": "index.html#overview",
    "title": "General linear models",
    "section": "Overview",
    "text": "Overview\nThis document focuses on demonstrating general linear models, with a particular emphasis on multiple regression. The penguins dataset from the palmerpenguins package in R, containing measurements of penguin species inhabiting the Palmer Archipelago, is used. The dataset was originally introduced by Gorman et al. (2014).\n\n\n\n\n\nFigure 1: Artwork by Allison Horst."
  },
  {
    "objectID": "index.html#question",
    "href": "index.html#question",
    "title": "General linear models",
    "section": "Question",
    "text": "Question\nCan Bill length and Bill depth predict Flipper length?\nThe goal is to test whether the variables bill_length_mm and bill_depth_mm are good predictors of flipper_length_mm. The definitions of these variables are provided below:\n\n\nbill_length_mm: numerical value representing bill length in millimeters.\n\nbill_depth_mm: numerical value representing bill depth in millimeters.\n\nflipper_length_mm: integer value representing flipper length in millimeters.\n\n\n\n\n\n\nFigure 2: Artwork by Allison Horst."
  },
  {
    "objectID": "index.html#a-brief-look-on-general-linear-models",
    "href": "index.html#a-brief-look-on-general-linear-models",
    "title": "General linear models",
    "section": "A brief look on general linear models",
    "text": "A brief look on general linear models\n\nSee DeGroot & Schervish (2012, pp. 699–707, pp. 736-754) and Hair (2019, pp. 259–370) to learn more.\n\n“[…] A problem of this type is called a problem of multiple linear regression because we are considering the regression of \\(Y\\) on \\(k\\) variables \\(X_{1}, \\dots, X_{k}\\), rather than on just a single variable \\(X\\), and we are assuming also that this regression is a linear function of the parameters \\(\\beta_{0}, \\dots, \\beta_{k}\\). In a problem of multiple linear regressions, we obtain \\(n\\) vectors of observations (\\(x_{i1}. \\dots, x_{ik}, Y_{i}\\)), for \\(i = 1, \\dots, n\\). Here \\(x_{ij}\\) is the observed value of the variable \\(X_{j}\\) for the \\(i\\)th observation. The \\(E(Y)\\) is given by the relation\n\\[\nE(Y_{i}) = \\beta_{0} + \\beta_{1} x_{i1} + \\dots + \\beta_{k} x_{ik}\n\\]\n(DeGroot & Schervish, 2012, p. 738)\nDefinitions\n\nResiduals/Fitted Values\n\nFor \\(i = 1, \\dots, n\\), the observed values of \\(\\hat{y} = \\hat{\\beta}_{0} + \\hat{\\beta}_{1} x_{i}\\) are called fitted values. For \\(i = 1, \\dots, n\\), the observed values of \\(e_{i} = y_{i} - \\hat{y}_{i}\\) are called residuals (DeGroot & Schervish, 2012, p. 717).\n\n\n“[…] regression problems in which the observations \\(Y_{i}, \\dots, Y_{n}\\) […] we shall assume that each observation \\(Y_{i}\\) has a normal distribution, that the observations \\(Y_{1}, \\dots, Y_{n}\\) are independent, and that the observations \\(Y_{1}, \\dots, Y_{n}\\) have the same variance \\(\\sigma^{2}\\). Instead of a single predictor being associated with each \\(Y_{i}\\), we assume that a \\(p\\)-dimensional vector \\(z_{i} = (z_{i0}, \\dots, z_{ip - 1})\\) is associated with each \\(Y_{i}\\)” (DeGroot & Schervish, 2012, p. 736).\n\nGeneral Linear Model\n\nThe statistical model in which the observations \\(Y_{1}, \\dots, Y_{n}\\) satisfy the following assumptions (DeGroot & Schervish, 2012, p. 738).\n\nAssumptions\n\nAssumption 1\n\nPredictor is known. Either the vectors \\(z_{1}, \\dots , z_{n}\\) are known ahead of time, or they are the observed values of random vectors \\(Z_{1}, \\dots , Z_{n}\\) on whose values we condition before computing the joint distribution of (\\(Y_{1}, \\dots , Y_{n}\\)) (DeGroot & Schervish, 2012, p. 736).\n\nAssumption 2\n\nNormality. For \\(i = 1, \\dots, n\\), the conditional distribution of \\(Y_{i}\\) given the vectors \\(z_{1}, \\dots , z_{n}\\) is a normal distribution (DeGroot & Schervish, 2012, p. 737).\n\n\n(Normality of the error term distribution (Hair, 2019, p. 287))\n\nAssumption 3\n\nLinear mean. There is a vector of parameters \\(\\beta = (\\beta_{0}, \\dots, \\beta_{p - 1})\\) such that the conditional mean of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) has the form\n\n\n\\[\nz_{i0} \\beta_{0} + z_{i1} \\beta_{1} + \\cdots + z_{ip - 1} \\beta_{p - 1}\n\\]\nfor \\(i = 1, \\dots, n\\) (DeGroot & Schervish, 2012, p. 737).\n(Linearity of the phenomenon measured (Hair, 2019, p. 287))\n\nAssumption 4\n\nCommon variance. There is as parameter \\(\\sigma^{2}\\) such the conditional variance of \\(Y_{i}\\) given the values \\(z_{1}, \\dots , z_{n}\\) is \\(\\sigma^{2}\\) for \\(i = 1, \\dots\\, n\\).\n\n\n(Constant variance of the error terms (Hair, 2019, p. 287))\n\nAssumption 5\n\nIndependence. The random variables \\(Y_{1}, \\dots , Y_{n}\\) are independent given the observed \\(z_{1}, \\dots , z_{n}\\) (DeGroot & Schervish, 2012, p. 737).\n\n\n(Independence of the error terms (Hair, 2019, p. 287))"
  },
  {
    "objectID": "index.html#setting-up-the-environment",
    "href": "index.html#setting-up-the-environment",
    "title": "General linear models",
    "section": "Setting up the environment",
    "text": "Setting up the environment\n\nCodelibrary(broom)\nlibrary(checkmate)\nlibrary(cowplot)\nlibrary(dplyr)\nlibrary(fBasics)\nlibrary(GGally)\nlibrary(ggplot2)\nlibrary(latex2exp)\nlibrary(magrittr)\nlibrary(moments)\nlibrary(nortest)\nlibrary(olsrr)\nlibrary(palmerpenguins)\nlibrary(parsnip)\nlibrary(recipes)\nlibrary(rutils)\nlibrary(stats)\nlibrary(tidyr)\nlibrary(tseries)\nlibrary(viridis)\nlibrary(workflows)\n\n\n\nCodetest_outlier &lt;- function(\n    x, \n    method = \"iqr\", \n    iqr_mult = 1.5, \n    sd_mult = 3\n  ) {\n  checkmate::assert_numeric(x)\n  checkmate::assert_choice(method, c(\"iqr\", \"sd\"))\n  checkmate::assert_number(iqr_mult)\n  checkmate::assert_number(sd_mult)\n\n  if (method == \"iqr\") {\n    iqr &lt;- stats::IQR(x, na.rm = TRUE)\n    min &lt;- stats::quantile(x, 0.25, na.rm = TRUE) - (iqr_mult * iqr)\n    max &lt;- stats::quantile(x, 0.75, na.rm = TRUE) + (iqr_mult * iqr)\n  } else if (method == \"sd\") {\n    min &lt;- mean(x, na.rm = TRUE) - (sd_mult * stats::sd(x, na.rm = TRUE))\n    max &lt;- mean(x, na.rm = TRUE) + (sd_mult * stats::sd(x, na.rm = TRUE))\n  }\n\n  dplyr::if_else(x &gt;= min & x &lt;= max, FALSE, TRUE, missing = FALSE)\n}\n\n\n\nCoderemove_outliers &lt;- function(\n    x, \n    method = \"iqr\", \n    iqr_mult = 1.5, \n    sd_mult = 3\n  ) {\n  checkmate::assert_numeric(x)\n  checkmate::assert_choice(method, c(\"iqr\", \"sd\"))\n  checkmate::assert_number(iqr_mult, lower = 1)\n  checkmate::assert_number(sd_mult, lower = 0)\n\n  x |&gt;\n    test_outlier(\n      method = method, \n      iqr_mult = iqr_mult, \n      sd_mult = sd_mult\n    ) %&gt;%\n    `!`() %&gt;%\n    magrittr::extract(x, .)\n}\n\n\n\nCodelist_as_tibble &lt;- function(list) {\n  checkmate::assert_list(list)\n\n  list |&gt;\n    dplyr::as_tibble() |&gt;\n    dplyr::mutate(\n      dplyr::across(\n        .cols = dplyr::everything(),\n        .fns = as.character\n      )\n    ) |&gt;\n    tidyr::pivot_longer(cols = dplyr::everything())\n}\n\n\n\nCodestats_sum &lt;- function(\n    x,\n    name = NULL,\n    na_rm = TRUE,\n    remove_outliers = FALSE,\n    iqr_mult = 1.5,\n    as_list = FALSE\n  ) {\n  checkmate::assert_numeric(x)\n  checkmate::assert_string(name, null.ok = TRUE)\n  checkmate::assert_flag(na_rm)\n  checkmate::assert_flag(remove_outliers)\n  checkmate::assert_number(iqr_mult, lower = 1)\n  checkmate::assert_flag(as_list)\n\n  if (isTRUE(remove_outliers)) {\n    x &lt;- x |&gt; remove_outliers(method = \"iqr\", iqr_mult = iqr_mult)\n  }\n\n  out &lt;- list(\n    n = length(x),\n    n_rm_na = length(x[!is.na(x)]),\n    n_na = length(x[is.na(x)]),\n    mean = mean(x, na.rm = na_rm),\n    var = stats::var(x, na.rm = na_rm),\n    sd = stats::sd(x, na.rm = na_rm),\n    min = rutils:::clear_names(stats::quantile(x, 0, na.rm = na_rm)),\n    q_1 = rutils:::clear_names(stats::quantile(x, 0.25, na.rm = na_rm)),\n    median = rutils:::clear_names(stats::quantile(x, 0.5, na.rm = na_rm)),\n    q_3 = rutils:::clear_names(stats::quantile(x, 0.75, na.rm = na_rm)),\n    max = rutils:::clear_names(stats::quantile(x, 1, na.rm = na_rm)),\n    iqr = IQR(x, na.rm = na_rm),\n    skewness = moments::skewness(x, na.rm = na_rm),\n    kurtosis = moments::kurtosis(x, na.rm = na_rm)\n  )\n\n  if (!is.null(name)) out &lt;- append(out, list(name = name), after = 0)\n  \n  if (isTRUE(as_list)) {\n    out\n  } else {\n    out |&gt; list_as_tibble()\n  }\n}\n\n\n\nCodeplot_qq &lt;- function(\n    x,\n    text_size = NULL,\n    na_rm = TRUE,\n    print = TRUE\n  ) {\n  checkmate::assert_numeric(x)\n  checkmate::assert_number(text_size, null.ok = TRUE)\n  checkmate::assert_flag(na_rm)\n  checkmate::assert_flag(print)\n\n  if (isTRUE(na_rm)) x &lt;- x |&gt; rutils:::drop_na()\n\n  plot &lt;-\n    dplyr::tibble(y = x) |&gt;\n    ggplot2::ggplot(ggplot2::aes(sample = y)) +\n    ggplot2::stat_qq() +\n    ggplot2::stat_qq_line(color = \"red\", linewidth = 1) +\n    ggplot2::labs(\n      x = \"Theoretical quantiles (Std. normal)\",\n      y = \"Sample quantiles\"\n    ) +\n    ggplot2::theme(text = ggplot2::element_text(size = text_size))\n\n  if (isTRUE(print)) print(plot)\n  \n  invisible(plot)\n}\n\n\n\nCodeplot_hist &lt;- function(\n    x,\n    name = \"x\",\n    bins = 30,\n    stat = \"density\",\n    text_size = NULL,\n    density_line = TRUE,\n    na_rm = TRUE,\n    print = TRUE\n  ) {\n  checkmate::assert_numeric(x)\n  checkmate::assert_string(name)\n  checkmate::assert_number(bins, lower = 1)\n  checkmate::assert_choice(stat, c(\"count\", \"density\"))\n  checkmate::assert_number(text_size, null.ok = TRUE)\n  checkmate::assert_flag(density_line)\n  checkmate::assert_flag(na_rm)\n  checkmate::assert_flag(print)\n\n  if (isTRUE(na_rm)) x &lt;- x |&gt; rutils:::drop_na()\n  y_lab &lt;- ifelse(stat == \"count\", \"Frequency\", \"Density\")\n\n  plot &lt;-\n    dplyr::tibble(y = x) |&gt;\n    ggplot2::ggplot(ggplot2::aes(x = y)) +\n    ggplot2::geom_histogram(\n      ggplot2::aes(y = ggplot2::after_stat(!!as.symbol(stat))),\n      bins = 30, \n      color = \"white\"\n    ) +\n    ggplot2::labs(x = name, y = y_lab) +\n    ggplot2::theme(text = ggplot2::element_text(size = text_size))\n\n  if (stat == \"density\" && isTRUE(density_line)) {\n    plot &lt;- plot + ggplot2::geom_density(color = \"red\", linewidth = 1)\n  }\n\n  if (isTRUE(print)) print(plot)\n  \n  invisible(plot)\n}\n\n\n\nCodeplot_ggally &lt;- function(\n    data,\n    cols = names(data),\n    mapping = NULL,\n    axis_labels = \"none\",\n    na_rm = TRUE,\n    text_size = NULL\n  ) {\n  checkmate::assert_tibble(data)\n  checkmate::assert_character(cols)\n  checkmate::assert_subset(cols, names(data))\n  checkmate::assert_class(mapping, \"uneval\", null.ok = TRUE)\n  checkmate::assert_choice(axis_labels, c(\"show\", \"internal\", \"none\"))\n  checkmate::assert_flag(na_rm)\n  checkmate::assert_number(text_size, null.ok = TRUE)\n\n  out &lt;-\n    data|&gt;\n    dplyr::select(dplyr::all_of(cols))|&gt;\n    dplyr::mutate(\n      dplyr::across(\n      .cols = dplyr::where(hms::is_hms),\n      .fns = ~ midday_trigger(.x)\n      ),\n      dplyr::across(\n        .cols = dplyr::where(\n          ~ !is.character(.x) && !is.factor(.x) && !is.numeric(.x)\n        ),\n        .fns = ~ as.numeric(.x)\n      )\n    )\n\n  if (isTRUE(na_rm)) out &lt;- out|&gt; tidyr::drop_na(dplyr::all_of(cols))\n\n  if (is.null(mapping)) {\n    plot &lt;-\n      out|&gt;\n      GGally::ggpairs(\n        lower = list(continuous = \"smooth\"),\n        axisLabels = axis_labels\n      ) \n  } else {\n    plot &lt;-\n      out|&gt;\n      GGally::ggpairs(\n        mapping = mapping,\n        axisLabels = axis_labels\n      ) +\n      viridis::scale_color_viridis(\n        begin = 0.25,\n        end = 0.75,\n        discrete = TRUE,\n        option = \"viridis\"\n      ) +\n      viridis::scale_fill_viridis(\n        begin = 0.25,\n        end = 0.75,\n        discrete = TRUE,\n        option = \"viridis\"\n      )\n  }\n\n  plot &lt;- \n    plot +\n    ggplot2::theme(text = ggplot2::element_text(size = text_size))\n\n  print(plot)\n  \n  invisible(plot)\n}\n\n\n\nCodetest_normality &lt;- function(x,\n                           name = \"x\",\n                           remove_outliers = FALSE,\n                           iqr_mult = 1.5,\n                           log_transform = FALSE,\n                           density_line = TRUE,\n                           text_size = NULL,\n                           print = TRUE) {\n  checkmate::assert_numeric(x)\n  checkmate::assert_string(name)\n  checkmate::assert_flag(remove_outliers)\n  checkmate::assert_number(iqr_mult, lower = 1)\n  checkmate::assert_flag(log_transform)\n  checkmate::assert_flag(density_line)\n  checkmate::assert_number(text_size, null.ok = TRUE)\n  checkmate::assert_flag(print)\n\n  n &lt;- x |&gt; length()\n  n_rm_na &lt;- x |&gt; rutils:::drop_na() |&gt; length()\n\n  if (isTRUE(remove_outliers)) {\n    x &lt;- x |&gt; remove_outliers(method = \"iqr\", iqr_mult = iqr_mult)\n  }\n\n  if (isTRUE(log_transform)) {\n    x &lt;-\n      x |&gt;\n      log() |&gt;\n      drop_inf()\n  }\n\n  if (n_rm_na &gt;= 7) {\n    ad &lt;- x |&gt; nortest::ad.test()\n\n    cvm &lt;-\n      x |&gt;\n      nortest::cvm.test() |&gt;\n      rutils:::shush()\n  } else {\n    ad &lt;- NULL\n    cmv &lt;- NULL\n  }\n\n  bonett &lt;- x |&gt; moments::bonett.test()\n\n  # See also `Rita::DPTest()` (just for Omnibus (K) tests).\n  dagostino &lt;-\n    x |&gt;\n    fBasics::dagoTest() |&gt;\n    rutils:::shush()\n\n  jarque_bera &lt;-\n    rutils:::drop_na(x) |&gt;\n    tseries::jarque.bera.test()\n\n  if (n_rm_na &gt;= 4) {\n    lillie_ks &lt;- x |&gt; nortest::lillie.test()\n  } else {\n    lillie_ks &lt;- NULL\n  }\n\n  pearson &lt;- x |&gt; nortest::pearson.test()\n\n  if (n_rm_na &gt;= 5 && n_rm_na &lt;= 5000) {\n    sf &lt;- x |&gt; nortest::sf.test()\n  } else {\n    sf &lt;- NULL\n  }\n\n  if (n_rm_na &gt;= 3 && n_rm_na &lt;= 3000) {\n    shapiro &lt;- x |&gt; stats::shapiro.test()\n  } else {\n    shapiro &lt;- NULL\n  }\n\n  qq_plot &lt;- x |&gt; plot_qq(text_size = text_size, print = FALSE)\n\n  hist_plot &lt;-\n    x |&gt;\n    plot_hist(\n      name = name,\n      text_size = text_size,\n      density_line = density_line,\n      print = FALSE\n      )\n\n  grid_plot &lt;- cowplot::plot_grid(hist_plot, qq_plot, ncol = 2, nrow = 1)\n\n  out &lt;- list(\n    stats = stats_sum(\n      x,\n      name = name,\n      na_rm = TRUE,\n      remove_outliers = FALSE,\n      as_list = TRUE\n    ),\n    params = list(\n      name = name,\n      remove_outliers = remove_outliers,\n      log_transform = log_transform,\n      density_line = density_line\n    ),\n\n    ad = ad,\n    bonett = bonett,\n    cvm = cvm,\n    dagostino = dagostino,\n    jarque_bera = jarque_bera,\n    lillie_ks = lillie_ks,\n    pearson = pearson,\n    sf = sf,\n    shapiro = shapiro,\n\n    hist_plot = hist_plot,\n    qq_plot = qq_plot,\n    grid_plot = grid_plot\n  )\n\n  if (isTRUE(print)) print(grid_plot)\n\n  invisible(out)\n}\n\n\n\nCodenormality_sum &lt;- function(x, round = FALSE, digits = 5, ...) {\n  checkmate::assert_numeric(x)\n  checkmate::assert_flag(round)\n  checkmate::assert_number(digits)\n\n  stats &lt;- test_normality(x, print = FALSE, ...)\n\n  out &lt;- dplyr::tibble(\n    test = c(\n      \"Anderson-Darling\",\n      \"Bonett-Seier\",\n      \"Cramer-von Mises\",\n      \"D'Agostino Omnibus Test\",\n      \"D'Agostino Skewness Test\",\n      \"D'Agostino Kurtosis Test\",\n      \"Jarque–Bera\",\n      \"Lilliefors (K-S)\",\n      \"Pearson chi-square\",\n      \"Shapiro-Francia\",\n      \"Shapiro-Wilk\"\n    ),\n    statistic_1 = c(\n      stats$ad$statistic,\n      stats$bonett$statistic[1],\n      stats$cvm$statistic,\n      attr(stats$dagostino, \"test\")$statistic[1],\n      attr(stats$dagostino, \"test\")$statistic[2],\n      attr(stats$dagostino, \"test\")$statistic[3],\n      stats$jarque_bera$statistic,\n      stats$lillie_ks$statistic,\n      stats$pearson$statistic,\n      ifelse(is.null(stats$shapiro), NA, stats$shapiro$statistic),\n      ifelse(is.null(stats$sf), NA, stats$sf$statistic)\n    ),\n    statistic_2 = c(\n      as.numeric(NA),\n      stats$bonett$statistic[2],\n      as.numeric(NA),\n      as.numeric(NA),\n      as.numeric(NA),\n      as.numeric(NA),\n      stats$jarque_bera$parameter,\n      as.numeric(NA),\n      as.numeric(NA),\n      as.numeric(NA),\n      as.numeric(NA)\n    ),\n    p_value = c(\n      stats$ad$p.value,\n      stats$bonett$p.value,\n      stats$cvm$p.value,\n      attr(stats$dagostino, \"test\")$p.value[1],\n      attr(stats$dagostino, \"test\")$p.value[2],\n      attr(stats$dagostino, \"test\")$p.value[3],\n      stats$jarque_bera$p.value,\n      stats$lillie_ks$p.value,\n      stats$pearson$p.value,\n      ifelse(is.null(stats$shapiro), NA, stats$shapiro$p.value),\n      ifelse(is.null(stats$sf), NA, stats$sf$p.value)\n    )\n  ) |&gt;\n    dplyr::select(test, p_value)\n\n  if (isTRUE(round)) {\n    out |&gt;\n      dplyr::mutate(\n        dplyr::across(\n          .cols = dplyr::where(is.numeric),\n          .fns = ~ round(.x, digits)\n        ))\n  } else {\n    out\n  }\n}"
  },
  {
    "objectID": "index.html#preparing-the-data",
    "href": "index.html#preparing-the-data",
    "title": "General linear models",
    "section": "Preparing the data",
    "text": "Preparing the data\n\nCodedata &lt;- \n  palmerpenguins::penguins |&gt; \n  dplyr::select(bill_length_mm, bill_depth_mm, flipper_length_mm) |&gt;\n  dplyr::mutate(\n    dplyr::across(\n      .cols = dplyr::everything(),\n      .fns = ~ dplyr::if_else(test_outlier(.x, , iqr_mult = 1), NA, .x)\n    )\n  ) |&gt;\n  tidyr::drop_na()\n\n\n\nCodedata"
  },
  {
    "objectID": "index.html#checking-variable-distributions",
    "href": "index.html#checking-variable-distributions",
    "title": "General linear models",
    "section": "Checking variable distributions",
    "text": "Checking variable distributions\n\n\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\n\n\n\nCodedata |&gt;\n  dplyr::pull(bill_length_mm) |&gt; \n  stats_sum(name = \"bill_length_mm\")\n\n\nTable 1: Statistics about the bill_length_mm variable.\n\n\n\n\n  \n\n\n\n\n\n\nCodedata |&gt; \n  dplyr::pull(bill_length_mm) |&gt; \n  test_normality(name = \"bill_length_mm\")\n#&gt; Registered S3 method overwritten by 'quantmod':\n#&gt;   method            from\n#&gt;   as.zoo.data.frame zoo\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 3: Histogram of the bill_length_mm variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\nCodedata |&gt; \n  dplyr::pull(bill_depth_mm) |&gt; \n  stats_sum(name = \"bill_depth_mm\")\n\n\nTable 2: Statistics about the bill_depth_mm variable.\n\n\n\n\n  \n\n\n\n\n\n\nCodedata |&gt; \n  dplyr::pull(bill_depth_mm) |&gt; \n  test_normality(name = \"bill_depth_mm\")\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4: Histogram of the bill_depth_mm variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution.\n\n\n\n\nCodedata |&gt; \n  dplyr::pull(flipper_length_mm) |&gt; \n  stats_sum(name = \"flipper_length_mm\")\n\n\nTable 3: Statistics about the flipper_length_mm variable.\n\n\n\n\n  \n\n\n\n\n\n\nCodedata |&gt; \n  dplyr::pull(flipper_length_mm) |&gt; \n  test_normality(name = \"flipper_length_mm\")\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 5: Histogram of the flipper_length_mm variable with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the variable and the theoretical quantiles of the normal distribution."
  },
  {
    "objectID": "index.html#checking-correlations",
    "href": "index.html#checking-correlations",
    "title": "General linear models",
    "section": "Checking correlations",
    "text": "Checking correlations\nCodedata |&gt; \n  plot_ggally() |&gt; \n  rutils:::shush()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 6: Correlation matrix of data variables."
  },
  {
    "objectID": "index.html#fitting-the-model",
    "href": "index.html#fitting-the-model",
    "title": "General linear models",
    "section": "Fitting the model",
    "text": "Fitting the model\n\nCoderecipe &lt;- \n  data %&gt;% \n  recipes::recipe(\n    flipper_length_mm ~ bill_length_mm + bill_depth_mm, \n    data = data\n  )\n\n\n\nCodemodel &lt;- \n  parsnip::linear_reg() |&gt; \n  parsnip::set_engine(\"lm\") |&gt;\n  parsnip::set_mode(\"regression\")\n\n\n\nCodeworkflow &lt;- \n  workflows::workflow() |&gt;\n  workflows::add_recipe(recipe) |&gt;\n  workflows::add_model(model)\n\n\n\nCodefit &lt;- workflow |&gt; parsnip::fit(data)\n\nfit\n#&gt; ══ Workflow [trained] ═══════════════════════════════════════════════════════\n#&gt; Preprocessor: Recipe\n#&gt; Model: linear_reg()\n#&gt; \n#&gt; ── Preprocessor ─────────────────────────────────────────────────────────────\n#&gt; 0 Recipe Steps\n#&gt; \n#&gt; ── Model ────────────────────────────────────────────────────────────────────\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Coefficients:\n#&gt;    (Intercept)  bill_length_mm   bill_depth_mm  \n#&gt;        191.365           1.464          -3.187\n\n\n\nCodebroom::tidy(fit)\n\n\n  \n\n\n\n\nCodebroom::augment(fit, data)\n\n\n  \n\n\n\n\nCodebroom::glance(fit) |&gt; tidyr::pivot_longer(cols = dplyr::everything())\n\n\n  \n\n\n\n\nCodefit_engine &lt;- fit |&gt; parsnip::extract_fit_engine()\n\nfit_engine |&gt; summary()\n#&gt; \n#&gt; Call:\n#&gt; stats::lm(formula = ..y ~ ., data = data)\n#&gt; \n#&gt; Residuals:\n#&gt;      Min       1Q   Median       3Q      Max \n#&gt; -22.8581  -5.7849  -0.0765   6.0064  23.2186 \n#&gt; \n#&gt; Coefficients:\n#&gt;                 Estimate Std. Error t value            Pr(&gt;|t|)    \n#&gt; (Intercept)    191.36528    6.25580   30.59 &lt;0.0000000000000002 ***\n#&gt; bill_length_mm   1.46397    0.08774   16.68 &lt;0.0000000000000002 ***\n#&gt; bill_depth_mm   -3.18659    0.23723  -13.43 &lt;0.0000000000000002 ***\n#&gt; ---\n#&gt; Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n#&gt; \n#&gt; Residual standard error: 8.392 on 337 degrees of freedom\n#&gt; Multiple R-squared:  0.6414, Adjusted R-squared:  0.6393 \n#&gt; F-statistic: 301.4 on 2 and 337 DF,  p-value: &lt; 0.00000000000000022\n\n\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  ggplot2::ggplot(ggplot2::aes(flipper_length_mm, .pred)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_abline(intercept = 0, slope = 1, color = \"red\") +\n  ggplot2::labs(x = \"Observed\", y = \"Predicted\") \n\n\n\n\n\n\n\n\n\n\n\n\nFigure 7: Relation between fitted and predicted values."
  },
  {
    "objectID": "index.html#performing-residual-diagnostics",
    "href": "index.html#performing-residual-diagnostics",
    "title": "General linear models",
    "section": "Performing residual diagnostics",
    "text": "Performing residual diagnostics\nNormality\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  stats_sum(name = \"Residuals\")\n\n\nTable 4: Statistics about the model residuals.\n\n\n\n\n  \n\n\n\n\n\n\nIt is important to note that the Kolmogorov-Smirnov and Pearson chi-square test are here just for reference since many authors don’t recommend using them when testing for normality (D’Agostino & Belanger, 1990).\nLearn more about normality tests in Thode (2002).\n\\[\n\\begin{cases}\n\\text{H}_{0}: \\text{Normality} \\\\\n\\text{H}_{a}: \\text{Nonnormality}\n\\end{cases}\n\\]\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  normality_sum()\n\n\nTable 5: Normality tests about the model residuals\n\n\n\n\n  \n\n\n\n\n\n\nCorrelation between observed residuals and expected residuals under normality.\n\nfit_engine |&gt; olsrr::ols_test_correlation()\n#&gt; [1] 0.9990882\n\nCodefit_engine |&gt;\n  stats::residuals() |&gt;\n  test_normality()\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 8: Histogram of the model residuals with a kernel density estimate, along with a quantile-quantile (Q-Q) plot between the residuals and the theoretical quantiles of the normal distribution\n\n\nCommon variance\nCodefit |&gt;\n  broom::augment(data) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_hline(yintercept = 0, color = \"red\", linewidth = 0.75) +\n  ggplot2::geom_smooth(formula = y ~ x, method = \"loess\", color = \"grey50\") +\n  ggplot2::labs(x = \"Fitted values\", y = \"Residuals\")\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 9: Relation between the fitted values of the restricted model and its residuals\n\n\n\nCode.resid_sd_error &lt;- \n  fit_engine |&gt;\n  stats::deviance() |&gt;\n  sqrt() %&gt;%\n  `/`(stats::df.residual(fit_engine))\n\n.resid_sd_error\n#&gt; [1] 0.4571229\n\n\nCodefit |&gt;\n  stats::predict(data) |&gt;\n  dplyr::mutate(\n    .sd_resid = \n      stats::rstandard(fit_engine) |&gt; \n      abs() |&gt;\n      sqrt()\n  ) |&gt;\n  ggplot2::ggplot(ggplot2::aes(.pred, .sd_resid)) +\n  ggplot2::geom_point() +\n  ggplot2::geom_smooth(formula = y ~ x, method = \"loess\", color = \"red\") +\n  ggplot2::labs(\n    x = \"Fitted values\", \n    y = latex2exp::TeX(\"$\\\\sqrt{Standardized \\\\ Residuals}$\")\n  )\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 10: Relation between the fitted values of the model and its standardized residuals\n\n\n\nCode# fit_engine |&gt; lmtest::bptest()\n\nfit_engine |&gt; olsrr::ols_test_breusch_pagan()\n#&gt; \n#&gt;  Breusch Pagan Test for Heteroskedasticity\n#&gt;  -----------------------------------------\n#&gt;  Ho: the variance is constant            \n#&gt;  Ha: the variance is not constant        \n#&gt; \n#&gt;              Data               \n#&gt;  -------------------------------\n#&gt;  Response : ..y \n#&gt;  Variables: fitted values of ..y \n#&gt; \n#&gt;         Test Summary          \n#&gt;  -----------------------------\n#&gt;  DF            =    1 \n#&gt;  Chi2          =    0.09240047 \n#&gt;  Prob &gt; Chi2   =    0.7611474\n\n\n\nCodefit_engine |&gt; olsrr::ols_test_score()\n#&gt; \n#&gt;  Score Test for Heteroskedasticity\n#&gt;  ---------------------------------\n#&gt;  Ho: Variance is homogenous\n#&gt;  Ha: Variance is not homogenous\n#&gt; \n#&gt;  Variables: fitted values of ..y \n#&gt; \n#&gt;         Test Summary         \n#&gt;  ----------------------------\n#&gt;  DF            =    1 \n#&gt;  Chi2          =    0.1075753 \n#&gt;  Prob &gt; Chi2   =    0.7429217\n\n\nIndependence\n\nVariance inflation factor (VIF)\n\n“Indicator of the effect that the other independent variables have on the standard error of a regression coefficient. The variance inflation factor is directly related to the tolerance value (\\(\\text{VIF}_{i} = 1/\\text{TO}L\\)). Large VIF values also indicate a high degree of collinearity or multicollinearity among the independent variables” (Hair, 2019, p. 265).\n\n\n\nfit_engine |&gt; olsrr::ols_coll_diag()\n#&gt; Tolerance and Variance Inflation Factor\n#&gt; ---------------------------------------\n#&gt;        Variables Tolerance      VIF\n#&gt; 1 bill_length_mm 0.9412773 1.062386\n#&gt; 2  bill_depth_mm 0.9412773 1.062386\n#&gt; \n#&gt; \n#&gt; Eigenvalue and Condition Index\n#&gt; ------------------------------\n#&gt;    Eigenvalue Condition Index    intercept bill_length_mm bill_depth_mm\n#&gt; 1 2.979217377         1.00000 0.0005948731    0.001539628   0.001378893\n#&gt; 2 0.017263355        13.13677 0.0002142314    0.414383774   0.343919362\n#&gt; 3 0.003519268        29.09544 0.9991908955    0.584076598   0.654701745\n\nMeasures of influence\n\nLeverage points\n\n“Type of influential observation defined by one aspect of influence termed leverage. These observations are substantially different on one or more independent variables, so that they affect the estimation of one or more regression coefficients” (Hair, 2019, p. 262).\n\n\n\n\n\nfit_engine |&gt; olsrr::ols_plot_resid_lev()\n\n\n\n\n\n\n\n\n\nFigure 11: Relation between the model studentized residuals and their leverage/influence points."
  },
  {
    "objectID": "index.html#references",
    "href": "index.html#references",
    "title": "General linear models",
    "section": "References",
    "text": "References\n\n\nD’Agostino, R. B., & Belanger, A. (1990). A suggestion for using powerful and informative tests of normality. The American Statistician, 44(4), 316–321. https://doi.org/10.2307/2684359\n\n\nDeGroot, M. H., & Schervish, M. J. (2012). Probability and statistics (4th ed.). Addison-Wesley.\n\n\nGorman, K. B., Williams, T. D., & Fraser, W. R. (2014). Ecological sexual dimorphism and environmental variability within a community of antarctic penguins (genus pygoscelis). PLOS ONE, 9(3), e90081. https://doi.org/10.1371/journal.pone.0090081\n\n\nHair, J. F. (2019). Multivariate data analysis (8th ed.). Cengage.\n\n\nThode, H. C. (2002). Testing for normality. Marcel Dekker."
  }
]